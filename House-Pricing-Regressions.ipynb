{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#mport tensorflow as  tf\n",
    "#mport tensorflow_decision_forests as tfdf\n",
    "import seaborn as sns\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df =pd.read_csv('sample_submission.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "dataset_df = pd.read_csv('train.csv')\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is composed of 81 columns and 1460 entries. We can see all 81 dimensions of our \n",
    "# dataset by printing out the first 3 entries using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 79 feature columns. Using these features your model has to predict the \n",
    "# house sale price indicated by the label column named SalePrice.\n",
    "# We will drop the Id column as it is not necessary for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.drop('Id', axis=1)\n",
    "dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#House Price Distribution\n",
    "#Now let us take a look at how the house prices are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_df['SalePrice'].describe())\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.distplot(dataset_df['SalePrice'], color='g', bins=100, hist_kws={'alpha' : 0.4});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical data distribution\n",
    "#We will now take a look at how the numerical features are distributed. \n",
    "# In order to do this, let us first list all the types of data from our dataset \n",
    "# and select only the numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(dataset_df.dtypes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = dataset_df.select_dtypes(include = ['float64', 'int64'])\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution for all the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the dataset\n",
    "#This dataset contains a mix of numeric, categorical and missing features.\n",
    "# TF-DF supports all these feature types natively, and no preprocessing is required.\n",
    "# This is one advantage of tree-based models, making them a great entry point to Tensorflow \n",
    "# and ML.\n",
    "\n",
    "#Now let us split the dataset into training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_dataset(dataset, test_ratio=0.30):\n",
    "    test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "    return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "train_ds_pd, valid_ds_pd = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "len(train_ds_pd), len(valid_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There's one more step required before we can train the model. We need to convert the datatset \n",
    "# from Pandas format (pd.DataFrame) into TensorFlow Datasets format (tf.data.Dataset).\n",
    "\n",
    "#TensorFlow Datasets is a high performance data loading library which is helpful when \n",
    "# training neural networks with accelerators like GPUs and TPUs.\n",
    "\n",
    "#By default the Random Forest Model is configured to train classification tasks. \n",
    "# Since this is a regression problem, we will specify the type of the task \n",
    "# (tfdf.keras.Task.REGRESSION) as a parameter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'SalePrice'\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label = label, task = tfdf.keras.Task.REGRESSION)\n",
    "valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label = label, task = tfdf.keras.Task.REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a Model\n",
    "# There are several tree-based models for you to choose from.\n",
    "\n",
    "    #RandomForestModel\n",
    "    #GradientBoostedTreesModel\n",
    "    #CartModel\n",
    "    #DistributedGradientBoostedTreesModel\n",
    "\n",
    "# To start, we'll work with a Random Forest. This is the most well-known of the Decision Forest training algorithms.\n",
    "\n",
    "# A Random Forest is a collection of decision trees, each trained independently on a random subset of the \n",
    "# training dataset (sampled with replacement). The algorithm is unique in that it is robust to overfitting, and easy to use.\n",
    "\n",
    "# We can list the all the available models in TensorFlow Decision Forests using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdf.keras.get_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to configure them:\n",
    "# TensorFlow Decision Forests provides good defaults for you (e.g. the top ranking hyperparameters on our benchmarks, \n",
    "# slightly modified to run in reasonable time). If you would like to configure the learning algorithm, \n",
    "# you will find many options you can explore to get the highest possible accuracy.\n",
    "# You can select a template and/or set parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = tfdf.keras.RandomForestModel(hyperparameter_template=\"benchmark_rank1\", task=tfdf.keras.Task.REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Random Forest\n",
    "# use the defaults to create the Random Forest Model while specifiyng the task type as tfdf.keras.Task.REGRESSION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)\n",
    "rf.compile(metrics=[\"mse\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# We will train the model using a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(x=train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model\n",
    "# One benefit of tree-based models is that you can easily visualize them. \n",
    "# The default number of trees used in the Random Forests is 300. We can select a tree to display below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the Out of bag (OOB) data and the validation dataset\n",
    "# Before training the dataset we have manually seperated 20% of the dataset for validation named as valid_ds.\n",
    "\n",
    "# We can also use Out of bag (OOB) score to validate our RandomForestModel. \n",
    "# To train a Random Forest Model, a set of random samples from training set are choosen by the algorithm \n",
    "# and the rest of the samples are used to finetune the model.The subset of data that is not chosen is known as \n",
    "# Out of bag data (OOB). OOB score is computed on the OOB data.\n",
    "\n",
    "# Read more about OOB data here.\n",
    "\n",
    "# The training logs show the Root Mean Squared Error (RMSE) evaluated on the out-of-bag dataset according to the number of trees in the model. Let us plot this.\n",
    "\n",
    "# Note: Smaller values are better for this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241m.\u001b[39mmake_inspector()\u001b[38;5;241m.\u001b[39mtraining_logs()\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([log\u001b[38;5;241m.\u001b[39mnum_trees \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m logs], [logs\u001b[38;5;241m.\u001b[39mevaluation\u001b[38;5;241m.\u001b[39mrmse \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m logs])\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Trees\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf' is not defined"
     ]
    }
   ],
   "source": [
    "logs = rf.make_inspector().training_logs()\n",
    "plt.plot([log.num_trees for log in logs], [logs.evaluation.rmse for log in logs])\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"RMSE (out-of-bag)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let us run an evaluation using the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = rf.evaluate(x=valid_ds, return_dict=True)\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importances\n",
    "# Variable importances indicate how much a feature contributes to the model predictions or quality. \n",
    "# There are several ways to identify important features using TensorFlow Decision Forests. \n",
    "# list the available Variable Importances for Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available variable importances:\")\n",
    "for importance in inspector.variable_importances().keys():\n",
    "    print(\"\\t\", importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, let us display the important features for the Variable Importance NUM_AS_ROOT.\n",
    "\n",
    "# The larger the importance score for NUM_AS_ROOT, the more impact it has on the outcome of the model.\n",
    "\n",
    "# By default, the list is sorted from the most important to the least. \n",
    "# From the output you can infer that the feature at the top of the list is used as the root node in most\n",
    "# number of trees in the random forest than any other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector.variable_importances()[\"NUM_AS_ROOT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variable importances from the inspector using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Mean decrease in AUC of the class 1 vs the others\n",
    "variable_importance_metric = \"NUM_AS_ROOT\"\n",
    "variable_importance_metric = inspector.variable_importances()[variable_importance_metric]\n",
    "\n",
    "# Extract the feature name and importance values\n",
    "#\n",
    "# variable importances is a list of <feature, importance> tuples.\n",
    "feature_names = [vi[0].name for vi in variable_importances]\n",
    "feature_importances = [vi[1] for vi in variable_importances]\n",
    "# The feature are ordered in decreasing importance value.\n",
    "feature_ranks = range(len(feature_names))\n",
    "\n",
    "bar = plt.barh(feature_ranks, feature_importances, label=[str(x) for x in feature_ranks])\n",
    "plt.yticks(feature_ranks, feature_names)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# todo: Replace with \"plt.bar_label()\" when available.\n",
    "# Label each bar with values\n",
    "for importance, patch in zip(feature_importances, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{importance:.4f}\", va=\"top\")\n",
    "\n",
    "plt.xlabel(variable_importance_metric)\n",
    "plt.title(\"NUM AS ROOT of the class 1 vs the others\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "# Predict on the competition test data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test_data.pop('Id')\n",
    "\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_data,task = tfdf.keras.Task.REGRESSION)\n",
    "\n",
    "preds = rf.predict(test_ds)\n",
    "output = pd.DataFrame({'Id': ids,'SalePrice': preds.squeeze()})\n",
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df['SalePrice'] = rf.predict(test_ds)\n",
    "sample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "sample_submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
